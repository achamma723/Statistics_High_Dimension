
@article{altenmullerWhenResearchMesearch2021,
  title = {When Research Is Me-Search: {{How}} Researchers’ Motivation to Pursue a Topic Affects Laypeople’s Trust in Science},
  shorttitle = {When Research Is Me-Search},
  author = {Altenmüller, Marlene Sophie and Lange, Leonie Lucia and Gollwitzer, Mario},
  date = {2021-07-09},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {7},
  pages = {e0253911},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0253911},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253911},
  urldate = {2022-01-12},
  abstract = {Research is often fueled by researchers’ scientific, but also their personal interests: Sometimes, researchers decide to pursue a specific research question because the answer to that question is idiosyncratically relevant for themselves: Such “me-search” may not only affect the quality of research, but also how it is perceived by the general public. In two studies (N = 621), we investigate the circumstances under which learning about a researcher’s “me-search” increases or decreases laypeople’s ascriptions of trustworthiness and credibility to the respective researcher. Results suggest that participants’ own preexisting attitudes towards the research topic moderate the effects of “me-search” substantially: When participants hold favorable attitudes towards the research topic (i.e., LGBTQ or veganism), “me-searchers” were perceived as more trustworthy and their research was perceived as more credible. This pattern was reversed when participants held unfavorable attitudes towards the research topic. Study 2 furthermore shows that trustworthiness and credibility perceptions generalize to evaluations of the entire field of research. Implications for future research and practice are discussed.},
  langid = {english},
  keywords = {Conflicts of interest,Homosexuals,Linear regression analysis,Motivation,Psychological attitudes,Psychology,Social sciences,Stroke},
  file = {/home/ahmad/Zotero/storage/P8PMANX3/Altenmüller et al. - 2021 - When research is me-search How researchers’ motiv.pdf;/home/ahmad/Zotero/storage/9UHDC8HN/article.html}
}

@article{altmannPermutationImportanceCorrected2010,
  title = {Permutation Importance: A Corrected Feature Importance Measure},
  shorttitle = {Permutation Importance},
  author = {Altmann, André and Toloşi, Laura and Sander, Oliver and Lengauer, Thomas},
  date = {2010-05-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {26},
  number = {10},
  pages = {1340--1347},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btq134},
  url = {https://doi.org/10.1093/bioinformatics/btq134},
  urldate = {2022-06-28},
  abstract = {Motivation: In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inflexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred.Results: In this work, we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias. The method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non-informative setting. The P-value of the observed importance provides a corrected measure of feature importance. We apply our method to simulated data and demonstrate that (i) non-informative predictors do not receive significant P-values, (ii) informative variables can successfully be recovered among non-informative variables and (iii) P-values computed with permutation importance (PIMP) are very helpful for deciding the significance of variables, and therefore improve model interpretability. Furthermore, PIMP was used to correct RF-based importance measures for two real-world case studies. We propose an improved RF model that uses the significant variables with respect to the PIMP measure and show that its prediction accuracy is superior to that of other existing models.Availability: R code for the method presented in this article is available at http://www.mpi-inf.mpg.de/∼altmann/download/PIMP.RContact:altmann@mpi-inf.mpg.de, laura.tolosi@mpi-inf.mpg.deSupplementary information:Supplementary data are available at Bioinformatics online.},
  file = {/home/ahmad/Zotero/storage/9LQS6FZ5/Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf;/home/ahmad/Zotero/storage/TN3647MX/193348.html}
}

@article{anaturkPredictionBrainAge2021,
  title = {Prediction of Brain Age and Cognitive Age: {{Quantifying}} Brain and Cognitive Maintenance in Aging},
  shorttitle = {Prediction of Brain Age and Cognitive Age},
  author = {Anatürk, Melis and Kaufmann, Tobias and Cole, James H. and Suri, Sana and Griffanti, Ludovica and Zsoldos, Enikő and Filippini, Nicola and Singh-Manoux, Archana and Kivimäki, Mika and Westlye, Lars T. and Ebmeier, Klaus P. and de Lange, Ann-Marie G.},
  options = {useprefix=true},
  date = {2021-04-15},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  volume = {42},
  number = {6},
  eprint = {33314530},
  eprinttype = {pmid},
  pages = {1626--1640},
  issn = {1097-0193},
  doi = {10.1002/hbm.25316},
  abstract = {The concept of brain maintenance refers to the preservation of brain integrity in older age, while cognitive reserve refers to the capacity to maintain cognition in the presence of neurodegeneration or aging-related brain changes. While both mechanisms are thought to contribute to individual differences in cognitive function among older adults, there is currently no "gold standard" for measuring these constructs. Using machine-learning methods, we estimated brain and cognitive age based on deviations from normative aging patterns in the Whitehall II MRI substudy cohort (N =\,537, age range = 60.34-82.76), and tested the degree of correspondence between these constructs, as well as their associations with premorbid IQ, education, and lifestyle trajectories. In line with established literature highlighting IQ as a proxy for cognitive reserve, higher premorbid IQ was linked to lower cognitive age independent of brain age. No strong evidence was found for associations between brain or cognitive age and lifestyle trajectories from midlife to late life based on latent class growth analyses. However, post hoc analyses revealed a relationship between cumulative lifestyle measures and brain age independent of cognitive age. In conclusion, we present a novel approach to characterizing brain and cognitive maintenance in aging, which may be useful for future studies seeking to identify factors that contribute to brain preservation and cognitive reserve mechanisms in older age.},
  langid = {english},
  pmcid = {PMC7978127},
  keywords = {Age Factors,Aged,Aged; 80 and over,aging,Aging,Brain,brain maintenance,cognitive reserve,Cognitive Reserve,Cohort Studies,Female,Humans,Intelligence,Life Style,lifestyle,machine learning,Machine Learning,Magnetic Resonance Imaging,Male,Middle Aged,neuroimaging,trajectories},
  file = {/home/ahmad/Zotero/storage/G4JLL47U/Anatürk et al. - 2021 - Prediction of brain age and cognitive age Quantif.pdf}
}

@article{barberControllingFalseDiscovery2015,
  title = {Controlling the False Discovery Rate via Knockoffs},
  author = {Barber, Rina Foygel and Candès, Emmanuel J.},
  date = {2015-10-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {43},
  number = {5},
  eprint = {1404.5609},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1337},
  url = {http://arxiv.org/abs/1404.5609},
  urldate = {2022-06-01},
  abstract = {In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, or the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap - their construction does not require any new data - and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/ahmad/Zotero/storage/MN2D5G6J/Barber and Candès - 2015 - Controlling the false discovery rate via knockoffs.pdf;/home/ahmad/Zotero/storage/NFJ5P5XJ/1404.html}
}

@article{baurleyBiosignatureDiscoverySubstance2018,
  title = {Biosignature {{Discovery}} for {{Substance Use Disorders Using Statistical Learning}}},
  author = {Baurley, James W. and McMahan, Christopher S. and Ervin, Carolyn M. and Pardamean, Bens and Bergen, Andrew W.},
  date = {2018-02-01},
  journaltitle = {Trends in Molecular Medicine},
  shortjournal = {Trends in Molecular Medicine},
  series = {Biomarkers of {{Substance Abuse}}},
  volume = {24},
  number = {2},
  pages = {221--235},
  issn = {1471-4914},
  doi = {10.1016/j.molmed.2017.12.008},
  url = {https://www.sciencedirect.com/science/article/pii/S1471491417302319},
  urldate = {2022-06-28},
  abstract = {There are limited biomarkers for substance use disorders (SUDs). Traditional statistical approaches are identifying simple biomarkers in large samples, but clinical use cases are still being established. High-throughput clinical, imaging, and ‘omic’ technologies are generating data from SUD studies and may lead to more sophisticated and clinically useful models. However, analytic strategies suited for high-dimensional data are not regularly used. We review strategies for identifying biomarkers and biosignatures from high-dimensional data types. Focusing on penalized regression and Bayesian approaches, we address how to leverage evidence from existing studies and knowledge bases, using nicotine metabolism as an example. We posit that big data and machine learning approaches will considerably advance SUD biomarker discovery. However, translation to clinical practice, will require integrated scientific efforts.},
  langid = {english},
  keywords = {artificial intelligence,biomarker,genomics,machine learning,nicotine metabolism,substance use disorders},
  file = {/home/ahmad/Zotero/storage/NCQA6KZG/Baurley et al. - 2018 - Biosignature Discovery for Substance Use Disorders.pdf}
}

@article{bradleyUseAreaROC1997,
  title = {The Use of the Area under the {{ROC}} Curve in the Evaluation of Machine Learning Algorithms},
  author = {Bradley, Andrew P.},
  date = {1997-07-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {30},
  number = {7},
  pages = {1145--1159},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(96)00142-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320396001422},
  urldate = {2022-01-12},
  abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
  langid = {english},
  keywords = {Accuracy measures,Cross-validation,Standard error,The area under the ROC curve (AUC),The ROC curve,Wilcoxon statistic},
  file = {/home/ahmad/Zotero/storage/SCJDZKYK/Bradley - 1997 - The use of the area under the ROC curve in the eva.pdf;/home/ahmad/Zotero/storage/MZVSG6FT/S0031320396001422.html}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001-10-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  url = {https://doi.org/10.1023/A:1010933404324},
  urldate = {2022-06-28},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {/home/ahmad/Zotero/storage/BYQ8Z75L/Breiman - 2001 - Random Forests.pdf}
}

@book{burzykowskiExplanatoryModelAnalysis,
  title = {Explanatory {{Model Analysis}}},
  author = {Burzykowski, Przemyslaw Biecek {and} Tomasz},
  url = {https://ema.drwhy.ai/},
  urldate = {2022-01-12},
  abstract = {This book introduces unified language for exploration, explanation and examination of predictive machine learning models.},
  file = {/home/ahmad/Zotero/storage/WQZKHJHZ/ema.drwhy.ai.html}
}

@article{bycroftUKBiobankResource2018,
  title = {The {{UK Biobank}} Resource with Deep Phenotyping and Genomic Data},
  author = {Bycroft, Clare and Freeman, Colin and Petkova, Desislava and Band, Gavin and Elliott, Lloyd T. and Sharp, Kevin and Motyer, Allan and Vukcevic, Damjan and Delaneau, Olivier and O’Connell, Jared and Cortes, Adrian and Welsh, Samantha and Young, Alan and Effingham, Mark and McVean, Gil and Leslie, Stephen and Allen, Naomi and Donnelly, Peter and Marchini, Jonathan},
  date = {2018-10},
  journaltitle = {Nature},
  volume = {562},
  number = {7726},
  pages = {203--209},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0579-z},
  url = {https://www.nature.com/articles/s41586-018-0579-z},
  urldate = {2022-06-28},
  abstract = {The UK Biobank project is a prospective cohort study with deep genetic and phenotypic data collected on approximately 500,000 individuals from across the United Kingdom, aged between 40 and 69 at recruitment. The open resource is unique in its size and scope. A rich variety of phenotypic and health-related information is available on each participant, including biological measurements, lifestyle indicators, biomarkers in blood and urine, and imaging of the body and brain. Follow-up information is provided by linking health and medical records. Genome-wide genotype data have been collected on all participants, providing many opportunities for the discovery of new genetic associations and the genetic bases of complex traits. Here we describe the centralized analysis of the genetic data, including genotype quality, properties of population structure and relatedness of the genetic data, and efficient phasing and genotype imputation that increases the number of testable variants to around 96~million. Classical allelic variation at 11 human leukocyte antigen genes was imputed, resulting in the recovery of signals with known associations between human leukocyte antigen alleles and many diseases.},
  issue = {7726},
  langid = {english},
  keywords = {Genome,Genome-wide association studies,Genotype,Haplotypes,Population genetics},
  file = {/home/ahmad/Zotero/storage/2BKZXQ6S/Bycroft et al. - 2018 - The UK Biobank resource with deep phenotyping and .pdf;/home/ahmad/Zotero/storage/SM7PMWZW/s41586-018-0579-z.html}
}

@unpublished{candesPanningGoldModelX2017,
  title = {Panning for {{Gold}}: {{Model-X Knockoffs}} for {{High-dimensional Controlled Variable Selection}}},
  shorttitle = {Panning for {{Gold}}},
  author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
  date = {2017-12-12},
  eprint = {1610.02351},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1610.02351},
  urldate = {2022-01-12},
  abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of \$model\$-\$X\$ knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand\textbackslash `es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with \$n\textbackslash ge p\$, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the \$controlled\$ variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/home/ahmad/Zotero/storage/YZ23F3Q5/Candes et al. - 2017 - Panning for Gold Model-X Knockoffs for High-dimen.pdf;/home/ahmad/Zotero/storage/ZSN64F6N/1610.html}
}

@inproceedings{casalicchioVisualizingFeatureImportance2019,
  title = {Visualizing the {{Feature Importance}} for {{Black Box Models}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Casalicchio, Giuseppe and Molnar, Christoph and Bischl, Bernd},
  editor = {Berlingerio, Michele and Bonchi, Francesco and Gärtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {655--670},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-10925-7_40},
  abstract = {In recent years, a large amount of model-agnostic methods to improve the transparency, trustability, and interpretability of machine learning models have been developed. Based on a recent method for model-agnostic global feature importance, we introduce a local feature importance measure for individual observations and propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Another contribution of our paper is the Shapley feature importance, which fairly distributes the overall performance of a model among the features according to the marginal contributions and which can be used to compare the feature importance across different models. Code related to this paper is available at: https://github.com/giuseppec/featureImportance.},
  isbn = {978-3-030-10925-7},
  langid = {english},
  keywords = {Explainable AI,Feature effect,Feature importance,Interpretable machine learning,Partial dependence,Variable importance},
  file = {/home/ahmad/Zotero/storage/R9X9RH67/Casalicchio et al. - 2019 - Visualizing the Feature Importance for Black Box M.pdf}
}

@article{castillo-barnesRobustEnsembleClassification2018,
  title = {Robust {{Ensemble Classification Methodology}} for {{I123-Ioflupane SPECT Images}} and {{Multiple Heterogeneous Biomarkers}} in the {{Diagnosis}} of {{Parkinson}}'s {{Disease}}},
  author = {Castillo-Barnes, Diego and Ramírez, Javier and Segovia, Fermín and Martínez-Murcia, Francisco J. and Salas-Gonzalez, Diego and Górriz, Juan M.},
  date = {2018},
  journaltitle = {Frontiers in Neuroinformatics},
  volume = {12},
  pages = {53},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00053},
  url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00053},
  urldate = {2022-01-12},
  abstract = {In last years, several approaches to develop an effective Computer-Aided-Diagnosis (CAD) system for Parkinson's Disease (PD) have been proposed. Most of these methods have focused almost exclusively on brain images through the use of Machine-Learning algorithms suitable to characterize structural or functional patterns. Those patterns provide enough information about the status and/or the progression at intermediate and advanced stages of Parkinson's Disease. Nevertheless this information could be insufficient at early stages of the pathology. The Parkinson's Progression Markers Initiative (PPMI) database includes neurological images along with multiple biomedical tests. This information opens up the possibility of comparing different biomarker classification results. As data come from heterogeneous sources, it is expected that we could include some of these biomarkers in order to obtain new information about the pathology. Based on that idea, this work presents an Ensemble Classification model with Performance Weighting. This proposal has been tested comparing Healthy Control subjects (HC) vs. patients with PD (considering both PD and SWEDD labeled subjects as the same class). This model combines several Support-Vector-Machine (SVM) with linear kernel classifiers for different biomedical group of tests—including CerebroSpinal Fluid (CSF), RNA, and Serum tests—and pre-processed neuroimages features (Voxels-As-Features and a list of defined Morphological Features) from PPMI database subjects. The proposed methodology makes use of all data sources and selects the most discriminant features (mainly from neuroimages). Using this performance-weighted ensemble classification model, classification results up to 96\% were obtained.},
  file = {/home/ahmad/Zotero/storage/SPJHTGSY/Castillo-Barnes et al. - 2018 - Robust Ensemble Classification Methodology for I12.pdf}
}

@unpublished{chammaVariableImportanceMedical2021,
  title = {Variable {{Importance}} on {{Medical Images}} and {{Socio-Demographic Data}}},
  author = {Chamma, Ahmad and Engemann, Denis and Thirion, Bertrand},
  date = {2021-12},
  url = {https://hal.archives-ouvertes.fr/hal-03480585},
  urldate = {2022-06-28},
  file = {/home/ahmad/Zotero/storage/KVPTQNM6/Chamma et al. - 2021 - Variable Importance on Medical Images and Socio-De.pdf}
}

@article{chipmanBARTBayesianAdditive2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  shorttitle = {{{BART}}},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  date = {2010-03-01},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {4},
  number = {1},
  eprint = {0806.3286},
  eprinttype = {arxiv},
  issn = {1932-6157},
  doi = {10.1214/09-AOAS285},
  url = {http://arxiv.org/abs/0806.3286},
  urldate = {2022-01-12},
  abstract = {We develop a Bayesian "sum-of-trees" model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/ahmad/Zotero/storage/R3FYGSN8/Chipman et al. - 2010 - BART Bayesian additive regression trees.pdf;/home/ahmad/Zotero/storage/CQTM7LQ2/0806.html}
}

@article{constantinescuFrameworkResearchContinental2022,
  title = {A Framework for Research into Continental Ancestry Groups of the {{UK Biobank}}},
  author = {Constantinescu, Andrei-Emil and Mitchell, Ruth E. and Zheng, Jie and Bull, Caroline J. and Timpson, Nicholas J. and Amulic, Borko and Vincent, Emma E. and Hughes, David A.},
  date = {2022-01-29},
  journaltitle = {Human Genomics},
  shortjournal = {Human Genomics},
  volume = {16},
  number = {1},
  pages = {3},
  issn = {1479-7364},
  doi = {10.1186/s40246-022-00380-5},
  url = {https://doi.org/10.1186/s40246-022-00380-5},
  urldate = {2022-06-28},
  abstract = {The UK Biobank is a large prospective cohort, based in the UK, that has deep phenotypic and genomic data on roughly a half a million individuals. Included in this resource are data on approximately 78,000 individuals with “non-white British ancestry.” While most epidemiology studies have focused predominantly on populations of European ancestry, there is an opportunity to contribute to the study of health and disease for a broader segment of the population by making use of the UK Biobank’s “non-white British ancestry” samples. Here, we present an empirical description of the continental ancestry and population structure among the individuals in this UK Biobank subset.},
  keywords = {Ancestry,Population structure,UK Biobank},
  file = {/home/ahmad/Zotero/storage/C9E65C3D/Constantinescu et al. - 2022 - A framework for research into continental ancestry.pdf;/home/ahmad/Zotero/storage/PCI74Z84/s40246-022-00380-5.html}
}

@article{coravosDevelopingAdoptingSafe2019,
  title = {Developing and Adopting Safe and Effective Digital Biomarkers to Improve Patient Outcomes},
  author = {Coravos, Andrea and Khozin, Sean and Mandl, Kenneth D.},
  date = {2019-03-11},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {2},
  number = {1},
  pages = {1--5},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0090-4},
  url = {https://www.nature.com/articles/s41746-019-0090-4},
  urldate = {2022-01-12},
  abstract = {Biomarkers are physiologic, pathologic, or anatomic characteristics that are objectively measured and evaluated as an indicator of normal biologic processes, pathologic processes, or biological responses to therapeutic interventions. Recent advances in the development of mobile digitally connected technologies have led to the emergence of a new class of biomarkers measured across multiple layers of hardware and software. Quantified in ones and zeros, these “digital” biomarkers can support continuous measurements outside the physical confines of the clinical environment. The modular software–hardware combination of these products has created new opportunities for patient care and biomedical research, enabling remote monitoring and decentralized clinical trial designs. However, a systematic approach to assessing the quality and utility of digital biomarkers to ensure an appropriate balance between their safety and effectiveness is needed. This paper outlines key considerations for the development and evaluation of digital biomarkers, examining their role in clinical research and routine patient care.},
  issue = {1},
  langid = {english},
  keywords = {Diagnostic markers,Policy},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Diagnostic markers;Policy Subject\_term\_id: diagnostic-markers;policy},
  file = {/home/ahmad/Zotero/storage/DQ8GESPK/Coravos et al. - 2019 - Developing and adopting safe and effective digital.pdf;/home/ahmad/Zotero/storage/WPCAVGVM/s41746-019-0090-4.html}
}

@article{cribbieEvaluatingImportanceIndividual2000,
  title = {Evaluating the Importance of Individual Parameters in Structural Equation Modeling: The Need for Type {{I}} Error Control},
  shorttitle = {Evaluating the Importance of Individual Parameters in Structural Equation Modeling},
  author = {Cribbie, Robert A.},
  date = {2000-09-01},
  journaltitle = {Personality and Individual Differences},
  shortjournal = {Personality and Individual Differences},
  volume = {29},
  number = {3},
  pages = {567--577},
  issn = {0191-8869},
  doi = {10.1016/S0191-8869(99)00219-6},
  url = {https://www.sciencedirect.com/science/article/pii/S0191886999002196},
  urldate = {2022-06-28},
  abstract = {The use of structural equation modeling in personality research has been increasing steadily over the past few decades. In evaluating the adequacy of a particular model researchers are often interested in evaluating not only the overall fit of the model, but also which of the proposed parameters are significant. Researchers who apply unrestricted post hoc model modifications, or who evaluate the significance of individual parameters without adopting some form of type I error control, risk capitalizing on chance. A Monte Carlo study was used to demonstrate the effectiveness of simple Bonferroni-type procedures for controlling the rate of type I errors when multiple parameters are evaluated in the structural portion of a theoretical model.},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/CJYIHQC5/Cribbie - 2000 - Evaluating the importance of individual parameters.pdf;/home/ahmad/Zotero/storage/RBXQ9X2P/S0191886999002196.html}
}

@article{dadiPopulationModelingMachine2021,
  title = {Population Modeling with Machine Learning Can Enhance Measures of Mental Health},
  author = {Dadi, Kamalaker and Varoquaux, Gaël and Houenou, Josselin and Bzdok, Danilo and Thirion, Bertrand and Engemann, Denis},
  date = {2021-09-24},
  pages = {2020.08.25.266536},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.08.25.266536},
  url = {https://www.biorxiv.org/content/10.1101/2020.08.25.266536v2},
  urldate = {2022-01-12},
  abstract = {Background Biological aging is revealed by physical measures, e.g., DNA probes or brain scans. Instead, individual differences in mental function are explained by psychological constructs, e.g., intelligence or neuroticism. These constructs are typically assessed by tailored neuropsychological tests that build on expert judgement and require careful interpretation. Could machine learning on large samples from the general population be used to build proxy measures of these constructs that do not require human intervention? Results Here, we built proxy measures by applying machine learning on multimodal MR images and rich sociodemographic information from the largest biomedical cohort to date: the UK Biobank. Objective model comparisons revealed that all proxies captured the target constructs and were as useful, and sometimes more useful than the original measures for characterizing real-world health behavior (sleep, exercise, tobacco, alcohol consumption). We observed this complementarity of proxy measures and original measures when modeling from brain signals or sociodemographic data, capturing multiple health-related constructs. Conclusions Population modeling with machine learning can derive measures of mental health from brain signals and questionnaire data, which may complement or even substitute for psychometric assessments in clinical populations. Key PointsWe applied machine learning on more than 10.000 individuals from the general population to define empirical approximations of health-related psychological measures that do not require human judgment.We found that machine-learning enriched the given psychological measures via approximation from brain and sociodemographic data: Resulting proxy measures related as well or better to real-world health behavior than the original measures.Model comparisons showed that sociodemographic information contributed most to characterizing psychological traits beyond aging.},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/XY4S7F2X/Dadi et al. - 2021 - Population modeling with machine learning can enha.pdf;/home/ahmad/Zotero/storage/SMHZ8URW/2020.08.25.html}
}

@article{denooijCognitiveFunctioningLifetime2020,
  title = {Cognitive Functioning and Lifetime Major Depressive Disorder in {{UK Biobank}}},
  author = {de Nooij, Laura and Harris, Mathew A. and Adams, Mark J. and Clarke, Toni-Kim and Shen, Xueyi and Cox, Simon R. and McIntosh, Andrew M. and Whalley, Heather C.},
  options = {useprefix=true},
  date = {2020-02-21},
  journaltitle = {European Psychiatry: The Journal of the Association of European Psychiatrists},
  shortjournal = {Eur Psychiatry},
  volume = {63},
  number = {1},
  eprint = {32189608},
  eprinttype = {pmid},
  pages = {e28},
  issn = {1778-3585},
  doi = {10.1192/j.eurpsy.2020.24},
  abstract = {BACKGROUND: Cognitive impairment associated with lifetime major depressive disorder (MDD) is well-supported by meta-analytic studies, but population-based estimates remain scarce. Previous UK Biobank studies have only shown limited evidence of cognitive differences related to probable MDD. Using updated cognitive and clinical assessments in UK Biobank, this study investigated population-level differences in cognitive functioning associated with lifetime MDD. METHODS: Associations between lifetime MDD and cognition (performance on six tasks and general cognitive functioning [g-factor]) were investigated in UK Biobank (N-range 7,457-14,836, age 45-81 years, 52\% female), adjusting for demographics, education, and lifestyle. Lifetime MDD classifications were based on the Composite International Diagnostic Interview. Within the lifetime MDD group, we additionally investigated relationships between cognition and (a) recurrence, (b) current symptoms, (c) severity of psychosocial impairment (while symptomatic), and (d) concurrent psychotropic medication use. RESULTS: Lifetime MDD was robustly associated with a lower g-factor (β~= -0.10, PFDR~= 4.7 × 10-5), with impairments in attention, processing speed, and executive functioning (β~≥ 0.06). Clinical characteristics revealed differential profiles of cognitive impairment among case individuals; those who reported severe psychosocial impairment and use of psychotropic medication performed worse on cognitive tests. Severe psychosocial impairment and reasoning showed the strongest association (β~= -0.18, PFDR~=~7.5 × 10-5). CONCLUSIONS: Findings describe small but robust associations between lifetime MDD and lower cognitive performance within a population-based sample. Overall effects were of modest effect size, suggesting limited clinical relevance. However, deficits within specific cognitive domains were more pronounced in relation to clinical characteristics, particularly severe psychosocial impairment.},
  langid = {english},
  pmcid = {PMC7315876},
  keywords = {Activities of Daily Living,Aged,Aged; 80 and over,Biological Specimen Banks,Cognition,Cognitive Dysfunction,Cross-Sectional Studies,depression,Depressive Disorder; Major,Executive Function,Female,Humans,Male,medication,Middle Aged,psychosocial functioning,UK Biobank,United Kingdom},
  file = {/home/ahmad/Zotero/storage/AN4XT29M/de Nooij et al. - 2020 - Cognitive functioning and lifetime major depressiv.pdf}
}

@article{fawns-ritchieReliabilityValidityUK2020,
  title = {Reliability and Validity of the {{UK Biobank}} Cognitive Tests},
  author = {Fawns-Ritchie, Chloe and Deary, Ian J.},
  date = {2020-04-20},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231627},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231627},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231627},
  urldate = {2022-01-13},
  abstract = {UK Biobank is a health resource with data from over 500,000 adults. The cognitive assessment in UK Biobank is brief and bespoke, and is administered without supervision on a touchscreen computer. Psychometric information on the UK Biobank cognitive tests are limited. Despite the non-standard nature of these tests and the limited psychometric information, the UK Biobank cognitive data have been used in numerous scientific publications. The present study examined the validity and short-term test-retest reliability of the UK Biobank cognitive tests. A sample of 160 participants (mean age = 62.59, SD = 10.24) was recruited who completed the UK Biobank cognitive assessment and a range of well-validated cognitive tests (‘reference tests’). Fifty-two participants returned 4 weeks later to repeat the UK Biobank tests. Correlations were calculated between UK Biobank tests and reference tests. Two measures of general cognitive ability were created by entering scores on the UK Biobank cognitive tests, and scores on the reference tests, respectively, into separate principal component analyses and saving scores on the first principal component. Four-week test-retest correlations were calculated for UK Biobank tests. UK Biobank cognitive tests showed a range of correlations with their respective reference tests, i.e. those tests that are thought to assess the same underlying cognitive ability (mean Pearson r = 0.53, range = 0.22 to 0.83, p≤.005). The measure of general cognitive ability based on the UK Biobank cognitive tests correlated at r = 0.83 (p {$<$} .001) with a measure of general cognitive ability created using the reference tests. Four-week test-retest reliability of the UK Biobank tests were moderate-to-high (mean Pearson r = 0.55, range = 0.40 to 0.89, p≤.003). Despite the brief, non-standard nature of the UK Biobank cognitive tests, some tests showed substantial concurrent validity and test-retest reliability. These psychometric results provide currently-lacking information on the validity of the UK Biobank cognitive tests.},
  langid = {english},
  keywords = {Cognition,Decision making,Intelligence tests,Memory,Principal component analysis,Psychometrics,Reaction time,Working memory},
  file = {/home/ahmad/Zotero/storage/LE4B2UAH/Fawns-Ritchie and Deary - 2020 - Reliability and validity of the UK Biobank cogniti.pdf;/home/ahmad/Zotero/storage/8XCTTG4A/article.html}
}

@online{HowWhyInterpret,
  title = {How and {{Why}} to {{Interpret Black Box Models}}},
  url = {https://www.elderresearch.com/blog/how-and-why-to-interpret-black-box-models/},
  urldate = {2022-06-28},
  abstract = {Blog reviews the application of interpretability methods to address calls for model accountability by improving model transparency and issue diagnostics, ethically serving the sometimes divergent needs of data scientists, regulators, and the public.},
  langid = {american},
  organization = {{Elder Research}},
  file = {/home/ahmad/Zotero/storage/Q5268TK3/how-and-why-to-interpret-black-box-models.html}
}

@article{hungImprovingPredictivePower2020,
  title = {Improving Predictive Power through Deep Learning Analysis of {{K-12}} Online Student Behaviors and Discussion Board Content},
  author = {Hung, Jui-Long and Rice, Kerry and Kepka, Jennifer and Yang, Juan},
  date = {2020-01-01},
  journaltitle = {Information Discovery and Delivery},
  volume = {48},
  number = {4},
  pages = {199--212},
  publisher = {{Emerald Publishing Limited}},
  issn = {2398-6247},
  doi = {10.1108/IDD-02-2020-0019},
  url = {https://doi.org/10.1108/IDD-02-2020-0019},
  urldate = {2022-06-28},
  abstract = {Purpose For studies in educational data mining or learning Analytics, the prediction of student’s performance or early warning is one of the most popular research topics. However, research gaps indicate a paucity of research using machine learning and deep learning (DL) models in predictive analytics that include both behaviors and text analysis. Design/methodology/approach This study combined behavioral data and discussion board content to construct early warning models with machine learning and DL algorithms. In total, 680 course sections, 12,869 students and 14,951,368 logs were collected from a K-12 virtual school in the USA. Three rounds of experiments were conducted to demonstrate the effectiveness of the proposed approach. Findings The DL model performed better than machine learning models and was able to capture 51\% of at-risk students in the eighth week with 86.8\% overall accuracy. The combination of behavioral and textual data further improved the model’s performance in both recall and accuracy rates. The total word count is a more general indicator than the textual content feature. Successful students showed more words in analytic, and at-risk students showed more words in authentic when text was imported into a linguistic function word analysis tool. The balanced threshold was 0.315, which can capture up to 59\% of at-risk students. Originality/value The results of this exploratory study indicate that the use of student behaviors and text in a DL approach may improve the predictive power of identifying at-risk learners early enough in the learning process to allow for interventions that can change the course of their trajectory.},
  keywords = {Deep learning,Early warning,Educational data mining,Educational text mining,Optimal threshold,Performance prediction},
  file = {/home/ahmad/Zotero/storage/HWBBPNNL/html.html}
}

@article{iniestaMachineLearningStatistical2016,
  title = {Machine Learning, Statistical Learning and the Future of Biological Research in Psychiatry},
  author = {Iniesta, R. and Stahl, D. and McGuffin, P.},
  date = {2016-09},
  journaltitle = {Psychological Medicine},
  volume = {46},
  number = {12},
  pages = {2455--2465},
  publisher = {{Cambridge University Press}},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291716001367},
  url = {https://www.cambridge.org/core/journals/psychological-medicine/article/machine-learning-statistical-learning-and-the-future-of-biological-research-in-psychiatry/7A6B51FCC58B10BDB8B51178AD52B50D},
  urldate = {2022-06-28},
  abstract = {Psychiatric research has entered the age of ‘Big Data’. Datasets now routinely involve thousands of heterogeneous variables, including clinical, neuroimaging, genomic, proteomic, transcriptomic and other ‘omic’ measures. The analysis of these datasets is challenging, especially when the number of measurements exceeds the number of individuals, and may be further complicated by missing data for some subjects and variables that are highly correlated. Statistical learning-based models are a natural extension of classical statistical approaches but provide more effective methods to analyse very large datasets. In addition, the predictive capability of such models promises to be useful in developing decision support systems. That is, methods that can be introduced to clinical settings and guide, for example, diagnosis classification or personalized treatment. In this review, we aim to outline the potential benefits of statistical learning methods in clinical research. We first introduce the concept of Big Data in different environments. We then describe how modern statistical learning models can be used in practice on Big Datasets to extract relevant information. Finally, we discuss the strengths of using statistical learning in psychiatric studies, from both research and practical clinical points of view.},
  langid = {english},
  keywords = {Machine learning,outcome prediction,personalized medicine,predictive modelling,statistical learning},
  file = {/home/ahmad/Zotero/storage/SM67DE6C/Iniesta et al. - 2016 - Machine learning, statistical learning and the fut.pdf}
}

@article{ishwaranVariableImportanceBinary2007,
  title = {Variable Importance in Binary Regression Trees and Forests},
  author = {Ishwaran, Hemant},
  date = {2007-01-01},
  journaltitle = {Electronic Journal of Statistics},
  shortjournal = {Electron. J. Statist.},
  volume = {1},
  eprint = {0711.2434},
  eprinttype = {arxiv},
  issn = {1935-7524},
  doi = {10.1214/07-EJS039},
  url = {http://arxiv.org/abs/0711.2434},
  urldate = {2022-01-12},
  abstract = {We characterize and study variable importance (VIMP) and pairwise variable associations in binary regression trees. A key component involves the node mean squared error for a quantity we refer to as a maximal subtree. The theory naturally extends from single trees to ensembles of trees and applies to methods like random forests. This is useful because while importance values from random forests are used to screen variables, for example they are used to filter high throughput genomic data in Bioinformatics, very little theory exists about their properties.},
  archiveprefix = {arXiv},
  issue = {none},
  keywords = {Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/7P8LIRT7/Ishwaran - 2007 - Variable importance in binary regression trees and.pdf;/home/ahmad/Zotero/storage/BLK8TXDE/0711.html}
}

@article{janitzaComputationallyFastVariable2018,
  title = {A Computationally Fast Variable Importance Test for Random Forests for High-Dimensional Data},
  author = {Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure},
  date = {2018-12-01},
  journaltitle = {Advances in Data Analysis and Classification},
  shortjournal = {Adv Data Anal Classif},
  volume = {12},
  number = {4},
  pages = {885--915},
  issn = {1862-5355},
  doi = {10.1007/s11634-016-0276-4},
  url = {https://doi.org/10.1007/s11634-016-0276-4},
  urldate = {2022-01-12},
  abstract = {Random forests are a commonly used tool for classification and for ranking candidate predictors based on the so-called variable importance measures. These measures attribute scores to the variables reflecting their importance. A drawback of variable importance measures is that there is no natural cutoff that can be used to discriminate between important and non-important variables. Several approaches, for example approaches based on hypothesis testing, were developed for addressing this problem. The existing testing approaches require the repeated computation of random forests. While for low-dimensional settings those approaches might be computationally tractable, for high-dimensional settings typically including thousands of candidate predictors, computing time is enormous. In this article a computationally fast heuristic variable importance test is proposed that is appropriate for high-dimensional data where many variables do not carry any information. The testing approach is based on a modified version of the permutation variable importance, which is inspired by cross-validation procedures. The new approach is tested and compared to the approach of Altmann and colleagues using simulation studies, which are based on real data from high-dimensional binary classification settings. The new approach controls the type I error and has at least comparable power at a substantially smaller computation time in the studies. Thus, it might be used as a computationally fast alternative to existing procedures for high-dimensional data settings where many variables do not carry any information. The new approach is implemented in the R package vita.},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/NDS5MIFG/Janitza et al. - 2018 - A computationally fast variable importance test fo.pdf}
}

@unpublished{kassaniDeepNeuralNetworks2021,
  title = {Deep Neural Networks with Controlled Variable Selection for the Identification of Putative Causal Genetic Variants},
  author = {Kassani, Peyman H. and Lu, Fred and Guen, Yann Le and He, Zihuai},
  date = {2021-09-29},
  eprint = {2109.14719},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2109.14719},
  urldate = {2022-01-13},
  abstract = {Deep neural networks (DNN) have been used successfully in many scientific problems for their high prediction accuracy, but their application to genetic studies remains challenging due to their poor interpretability. In this paper, we consider the problem of scalable, robust variable selection in DNN for the identification of putative causal genetic variants in genome sequencing studies. We identified a pronounced randomness in feature selection in DNN due to its stochastic nature, which may hinder interpretability and give rise to misleading results. We propose an interpretable neural network model, stabilized using ensembling, with controlled variable selection for genetic studies. The merit of the proposed method includes: (1) flexible modelling of the non-linear effect of genetic variants to improve statistical power; (2) multiple knockoffs in the input layer to rigorously control false discovery rate; (3) hierarchical layers to substantially reduce the number of weight parameters and activations to improve computational efficiency; (4) de-randomized feature selection to stabilize identified signals. We evaluated the proposed method in extensive simulation studies and applied it to the analysis of Alzheimer disease genetics. We showed that the proposed method, when compared to conventional linear and nonlinear methods, can lead to substantially more discoveries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/ZJ9TW8BX/Kassani et al. - 2021 - Deep neural networks with controlled variable sele.pdf;/home/ahmad/Zotero/storage/MCABMUDY/2109.html}
}

@unpublished{konigRelativeFeatureImportance2021,
  title = {Relative {{Feature Importance}}},
  author = {König, Gunnar and Molnar, Christoph and Bischl, Bernd and Grosse-Wentrup, Moritz},
  date = {2021},
  volume = {12667},
  eprint = {2007.08283},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1007/978-3-030-68787-8},
  url = {http://arxiv.org/abs/2007.08283},
  urldate = {2022-05-12},
  abstract = {Interpretable Machine Learning (IML) methods are used to gain insight into the relevance of a feature of interest for the performance of a model. Commonly used IML methods differ in whether they consider features of interest in isolation, e.g., Permutation Feature Importance (PFI), or in relation to all remaining feature variables, e.g., Conditional Feature Importance (CFI). As such, the perturbation mechanisms inherent to PFI and CFI represent extreme reference points. We introduce Relative Feature Importance (RFI), a generalization of PFI and CFI that allows for a more nuanced feature importance computation beyond the PFI versus CFI dichotomy. With RFI, the importance of a feature relative to any other subset of features can be assessed, including variables that were not available at training time. We derive general interpretation rules for RFI based on a detailed theoretical analysis of the implications of relative feature relevance, and demonstrate the method's usefulness on simulated examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/HJ4IPPD6/König et al. - 2021 - Relative Feature Importance.pdf;/home/ahmad/Zotero/storage/N9427HVR/2007.html}
}

@inproceedings{kumarProblemsShapleyvaluebasedExplanations2020,
  title = {Problems with {{Shapley-value-based}} Explanations as Feature Importance Measures},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
  date = {2020-11-21},
  pages = {5491--5500},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/kumar20e.html},
  urldate = {2022-01-13},
  abstract = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game’s unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/PDF27XRC/Kumar et al. - 2020 - Problems with Shapley-value-based explanations as .pdf}
}

@article{leeMachineLearningEnterprises2020,
  title = {Machine Learning for Enterprises: {{Applications}}, Algorithm Selection, and Challenges},
  shorttitle = {Machine Learning for Enterprises},
  author = {Lee, In and Shin, Yong Jae},
  date = {2020-03-01},
  journaltitle = {Business Horizons},
  shortjournal = {Business Horizons},
  series = {{{ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING}}},
  volume = {63},
  number = {2},
  pages = {157--170},
  issn = {0007-6813},
  doi = {10.1016/j.bushor.2019.10.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0007681319301521},
  urldate = {2022-01-12},
  abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.},
  langid = {english},
  keywords = {Artificial intelligence,Big data,Chatbot,Deep learning,Innovation capability,Machine learning,Neural networks,Resources and capabilities},
  file = {/home/ahmad/Zotero/storage/J6TX73T2/S0007681319301521.html}
}

@unpublished{liuFastPowerfulConditional2021,
  title = {Fast and {{Powerful Conditional Randomization Testing}} via {{Distillation}}},
  author = {Liu, Molei and Katsevich, Eugene and Janson, Lucas and Ramdas, Aaditya},
  date = {2021-06-04},
  eprint = {2006.03980},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/2006.03980},
  urldate = {2022-01-12},
  abstract = {We consider the problem of conditional independence testing: given a response Y and covariates (X,Z), we test the null hypothesis that Y is independent of X given Z. The conditional randomization test (CRT) was recently proposed as a way to use distributional information about X|Z to exactly (non-asymptotically) control Type-I error using any test statistic in any dimensionality without assuming anything about Y|(X,Z). This flexibility in principle allows one to derive powerful test statistics from complex prediction algorithms while maintaining statistical validity. Yet the direct use of such advanced test statistics in the CRT is prohibitively computationally expensive, especially with multiple testing, due to the CRT's requirement to recompute the test statistic many times on resampled data. We propose the distilled CRT, a novel approach to using state-of-the-art machine learning algorithms in the CRT while drastically reducing the number of times those algorithms need to be run, thereby taking advantage of their power and the CRT's statistical guarantees without suffering the usual computational expense. In addition to distillation, we propose a number of other tricks like screening and recycling computations to further speed up the CRT without sacrificing its high power and exact validity. Indeed, we show in simulations that all our proposals combined lead to a test that has similar power to the most powerful existing CRT implementations but requires orders of magnitude less computation, making it a practical tool even for large data sets. We demonstrate these benefits on a breast cancer dataset by identifying biomarkers related to cancer stage.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/ahmad/Zotero/storage/8HRQZX3H/Liu et al. - 2021 - Fast and Powerful Conditional Randomization Testin.pdf;/home/ahmad/Zotero/storage/YFNDKN2B/2006.html}
}

@article{louppeUnderstandingVariableImportances,
  title = {Understanding Variable Importances in Forests of Randomized Trees},
  author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
  pages = {9},
  abstract = {Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of non-totally randomized trees such as Random Forests and Extra-Trees.},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/HUZLPFJ7/Louppe et al. - Understanding variable importances in forests of r.pdf}
}

@unpublished{luDeepPINKReproducibleFeature2018,
  title = {{{DeepPINK}}: Reproducible Feature Selection in Deep Neural Networks},
  shorttitle = {{{DeepPINK}}},
  author = {Lu, Yang Young and Fan, Yingying and Lv, Jinchi and Noble, William Stafford},
  date = {2018-09-06},
  eprint = {1809.01185},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.01185},
  urldate = {2022-01-13},
  abstract = {Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/TRA4SI37/Lu et al. - 2018 - DeepPINK reproducible feature selection in deep n.pdf;/home/ahmad/Zotero/storage/PDKFPWLS/1809.html}
}

@online{MachineLearningApplications,
  title = {Machine Learning Applications in Medical Image Analysis},
  url = {https://www.nature.com/collections/gfbjhfjfgg/},
  urldate = {2022-05-24},
  abstract = {This Collection aims to bring together original research on all aspects of ML-based medical image analysis, including technological developments and new ...},
  langid = {english},
  organization = {{Nature}},
  file = {/home/ahmad/Zotero/storage/EBXTL28F/gfbjhfjfgg.html}
}

@online{MachineLearningHealthcare,
  title = {Machine {{Learning}} in {{Healthcare Data Analysis}}: {{A Survey}}},
  url = {https://www.iomcworld.org/articles/machine-learning-in-healthcare-data-analysis-a-survey-44184.html},
  urldate = {2022-06-28},
  file = {/home/ahmad/Zotero/storage/QQKQFWPC/machine-learning-in-healthcare-data-analysis-a-survey-44184.html}
}

@book{malleyStatisticalLearningBiomedical2011,
  title = {Statistical {{Learning}} for {{Biomedical Data}}},
  author = {Malley, James D. and Malley, Karen G. and Pajevic, Sinisa},
  date = {2011-02-24},
  eprint = {WvoLsY6bYNwC},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {This book is for anyone who has biomedical data and needs to identify variables that predict an outcome, for two-group outcomes such as tumor/not-tumor, survival/death, or response from treatment. Statistical learning machines are ideally suited to these types of prediction problems, especially if the variables being studied may not meet the assumptions of traditional techniques. Learning machines come from the world of probability and computer science but are not yet widely used in biomedical research. This introduction brings learning machine techniques to the biomedical world in an accessible way, explaining the underlying principles in nontechnical language and using extensive examples and figures. The authors connect these new methods to familiar techniques by showing how to use the learning machine models to generate smaller, more easily interpretable traditional models. Coverage includes single decision trees, multiple-tree techniques such as Random ForestsTM, neural nets, support vector machines, nearest neighbors and boosting.},
  isbn = {978-1-139-49685-8},
  langid = {english},
  pagetotal = {301},
  keywords = {Mathematics / Probability & Statistics / General,Medical / Biostatistics,Medical / Epidemiology}
}

@article{miPermutationbasedIdentificationImportant2021,
  title = {Permutation-Based Identification of Important Biomarkers for Complex Diseases via Machine Learning Models},
  author = {Mi, Xinlei and Zou, Baiming and Zou, Fei and Hu, Jianhua},
  date = {2021-05-21},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {3008},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22756-2},
  url = {https://www.nature.com/articles/s41467-021-22756-2},
  urldate = {2022-01-12},
  abstract = {Study of human disease remains challenging due to convoluted disease etiologies and complex molecular mechanisms at genetic, genomic, and proteomic levels. Many machine learning-based methods have been developed and widely used to alleviate some analytic challenges in complex human disease studies. While enjoying the modeling flexibility and robustness, these model frameworks suffer from non-transparency and difficulty in interpreting each individual feature due to their sophisticated algorithms. However, identifying important biomarkers is a critical pursuit towards assisting researchers to establish novel hypotheses regarding prevention, diagnosis and treatment of complex human diseases. Herein, we propose a Permutation-based Feature Importance Test (PermFIT) for estimating and testing the feature importance, and for assisting interpretation of individual feature in complex frameworks, including deep neural networks, random forests, and support vector machines. PermFIT (available at https://github.com/SkadiEye/deepTL) is implemented in a computationally efficient manner, without model refitting. We conduct extensive numerical studies under various scenarios, and show that PermFIT not only yields valid statistical inference, but also improves the prediction accuracy of machine learning models. With the application to the Cancer Genome Atlas kidney tumor data and the HITChip atlas data, PermFIT demonstrates its practical usage in identifying important biomarkers and boosting model prediction performance.},
  issue = {1},
  langid = {english},
  keywords = {Cancer,Data mining,Machine learning,Statistical methods},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cancer;Data mining;Machine learning;Statistical methods Subject\_term\_id: cancer;data-mining;machine-learning;statistical-methods},
  file = {/home/ahmad/Zotero/storage/AL4ZN6J6/Mi et al. - 2021 - Permutation-based identification of important biom.pdf;/home/ahmad/Zotero/storage/MXTSY7AF/s41467-021-22756-2.html}
}

@article{mitchellNationalBiobanksClinical2010,
  title = {National {{Biobanks}}: {{Clinical Labor}}, {{Risk Production}}, and the {{Creation}} of {{Biovalue}}},
  shorttitle = {National {{Biobanks}}},
  author = {Mitchell, Robert and Waldby, Catherine},
  date = {2010-05-01},
  journaltitle = {Science, Technology, \& Human Values},
  shortjournal = {Science, Technology, \& Human Values},
  volume = {35},
  number = {3},
  pages = {330--355},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/0162243909340267},
  url = {https://doi.org/10.1177/0162243909340267},
  urldate = {2022-01-12},
  abstract = {The development of genomics has dramatically expanded the scope of genetic research, and collections of genetic biosamples have proliferated in countries with active genomics research programs. In this essay, we consider a particular kind of collection, national biobanks. National biobanks are often presented by advocates as an economic ‘‘resource’’ that will be used by both basic researchers and academic biologists, as well as by pharmaceutical diagnostic and clinical genomics companies. Although national biobanks have been the subject of intense interest in recent social science literature, most prior work on this topic focuses either on bioethical issues related to biobanks, such as the question of informed consent, or on the possibilities for scientific citizenship that they make possible. We emphasize, by contrast, the economic aspect of biobanks, focusing specifically on the way in which national biobanks create biovalue. Our emphasis on the economic aspect of biobanks allows us to recognize the importance of what we call clinical labor—that is, the regularized, embodied work that members of the national population are expected to perform in their role as biobank participants—in the creation of biovalue through biobanks. Moreover, it allows us to understand how the technical way in which national biobanks link clinical labor to databases alters both medical and popular understandings of risk for common diseases and conditions.},
  langid = {english},
  keywords = {biobank,biovalue,clinical labor,database,risk},
  file = {/home/ahmad/Zotero/storage/D498GWHF/Mitchell and Waldby - 2010 - National Biobanks Clinical Labor, Risk Production.pdf}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/},
  urldate = {2022-06-28},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/home/ahmad/Zotero/storage/NQZ6ZSRS/interpretable-ml-book.html}
}

@book{molnarPermutationFeatureImportance,
  title = {8.5 {{Permutation Feature Importance}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/feature-importance.html},
  urldate = {2022-06-01},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/home/ahmad/Zotero/storage/ILZC7V94/feature-importance.html}
}

@article{mutzLifetimeDepressionAgerelated2021,
  title = {Lifetime Depression and Age-Related Changes in Body Composition, Cardiovascular Function, Grip Strength and Lung Function: Sex-Specific Analyses in the {{UK Biobank}}},
  shorttitle = {Lifetime Depression and Age-Related Changes in Body Composition, Cardiovascular Function, Grip Strength and Lung Function},
  author = {Mutz, Julian and Lewis, Cathryn M.},
  date = {2021-07-07},
  journaltitle = {Aging},
  shortjournal = {Aging (Albany NY)},
  volume = {13},
  number = {13},
  eprint = {34233295},
  eprinttype = {pmid},
  pages = {17038--17079},
  issn = {1945-4589},
  doi = {10.18632/aging.203275},
  abstract = {Individuals with depression, on average, die prematurely, have high levels of physical comorbidities and may experience accelerated biological ageing. A greater understanding of age-related changes in physiology could provide novel biological insights that may help inform strategies to mitigate excess mortality in depression. We used generalised additive models to examine age-related changes in 15 cardiovascular, body composition, grip strength and lung function measures, comparing males and females with a lifetime history of depression to healthy controls. The main dataset included 342,393 adults (mean age = 55.87 years, SD = 8.09; 52.61\% females). We found statistically significant case-control differences for most physiological measures. There was some evidence that age-related changes in body composition, cardiovascular function, lung function and heel bone mineral density followed different trajectories in depression. These differences did not uniformly narrow or widen with age and differed by sex. For example, BMI in female cases was 1.1 kg/m2 higher at age 40 and this difference narrowed to 0.4 kg/m2 at age 70. In males, systolic blood pressure was 1 mmHg lower in depression cases at age 45 and this difference widened to 2.5 mmHg at age 65. These findings suggest that targeted screening for physiological function in middle-aged and older adults with depression is warranted to potentially mitigate excess mortality.},
  langid = {english},
  pmcid = {PMC8312429},
  keywords = {Adult,Aged,aging,Aging,Biological Specimen Banks,Blood Pressure,body composition,Body Composition,Body Mass Index,Bone Density,cardiovascular function,Cardiovascular Physiological Phenomena,Case-Control Studies,depression,Depression,Female,grip strength,Hand Strength,Humans,Male,Middle Aged,Respiratory Function Tests,Sex Characteristics,United Kingdom,Vascular Stiffness},
  file = {/home/ahmad/Zotero/storage/U85N6U6Y/Mutz and Lewis - 2021 - Lifetime depression and age-related changes in bod.pdf}
}

@article{newbyAssociationsBrainVolumes2021,
  title = {Associations {{Between Brain Volumes}} and {{Cognitive Tests}} with {{Hypertensive Burden}} in {{UK Biobank}}},
  author = {Newby, Danielle and Winchester, Laura and Sproviero, William and Fernandes, Marco and Wang, Dai and Kormilitzin, Andrey and Launer, Lenore J. and Nevado-Holgado, Alejo J.},
  date = {2021-01-01},
  journaltitle = {Journal of Alzheimer's Disease},
  volume = {84},
  number = {3},
  pages = {1373--1389},
  publisher = {{IOS Press}},
  issn = {1387-2877},
  doi = {10.3233/JAD-210512},
  url = {https://content.iospress.com/articles/journal-of-alzheimers-disease/jad210512},
  urldate = {2022-01-13},
  abstract = {Background: Mid-life hypertension is an established risk factor for cognitive impairment and dementia and related to greater brain atrophy and poorer cognitive performance. Previous studies often have small sample sizes from older populations that la},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/5U9NQCHC/Newby et al. - 2021 - Associations Between Brain Volumes and Cognitive T.pdf;/home/ahmad/Zotero/storage/SLS3QAA4/jad210512.html}
}

@unpublished{nguyenAggregationMultipleKnockoffs2020,
  title = {Aggregation of {{Multiple Knockoffs}}},
  author = {Nguyen, Tuan-Binh and Chevalier, Jérôme-Alexis and Thirion, Bertrand and Arlot, Sylvain},
  date = {2020-06-25},
  eprint = {2002.09269},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/2002.09269},
  urldate = {2022-02-09},
  abstract = {We develop an extension of the Knockoff Inference procedure, introduced by Barber and Candes (2015). This new method, called Aggregation of Multiple Knockoffs (AKO), addresses the instability inherent to the random nature of Knockoff-based inference. Specifically, AKO improves both the stability and power compared with the original Knockoff algorithm while still maintaining guarantees for False Discovery Rate control. We provide a new inference procedure, prove its core properties, and demonstrate its benefits in a set of experiments on synthetic and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/ahmad/Zotero/storage/JF5ZS2VM/Nguyen et al. - 2020 - Aggregation of Multiple Knockoffs.pdf;/home/ahmad/Zotero/storage/LN8GCPXT/2002.html}
}

@article{nicodemusBehaviourRandomForest2010,
  title = {The Behaviour of Random Forest Permutation-Based Variable Importance Measures under Predictor Correlation},
  author = {Nicodemus, Kristin K. and Malley, James D. and Strobl, Carolin and Ziegler, Andreas},
  date = {2010-02-27},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {110},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-110},
  url = {https://doi.org/10.1186/1471-2105-11-110},
  urldate = {2022-06-28},
  abstract = {Random forests (RF) have been increasingly used in applications such as genome-wide association and microarray studies where predictor correlation is frequently observed. Recent works on permutation-based variable importance measures (VIMs) used in RF have come to apparently contradictory conclusions. We present an extended simulation study to synthesize results.},
  keywords = {Bivariate Model,Correlate Predictor,Correlate Variable,Linear Regression Model,Random Forest},
  file = {/home/ahmad/Zotero/storage/UPFMVK2X/Nicodemus et al. - 2010 - The behaviour of random forest permutation-based v.pdf;/home/ahmad/Zotero/storage/7BYLG7HN/1471-2105-11-110.html}
}

@article{olatayoResidualBootstrapMethod2011,
  title = {Residual Bootstrap Method: An Approach to Model Validation},
  shorttitle = {Residual Bootstrap Method},
  author = {Olatayo, Timothy and Oredein, A.I. and Abayomi, Akomolafe},
  date = {2011-01-01},
  journaltitle = {Advances and Applications in Statistics},
  shortjournal = {Advances and Applications in Statistics},
  volume = {25},
  abstract = {Model validation is an important step in the modeling process and helps in assessing the reliability of models before they can be used in decision making. We used residual bootstrap method to study regression model validation process. Various validation statistic such as the standard error (SE), mean square error (MSE), and coefficient of determination (R 2 ) were used as criteria for selecting the best model. The values of these statistics revealed that the residual bootstrap approach gives better estimates, which reduces the risk of over fitted models.}
}

@article{pengPracticalIndicatorsRisk2022,
  title = {Practical {{Indicators}} for {{Risk}} of {{Airborne Transmission}} in {{Shared Indoor Environments}} and {{Their Application}} to {{COVID-19 Outbreaks}}},
  author = {Peng, Z. and Rojas, A.L. Pineda and Kropff, E. and Bahnfleth, W. and Buonanno, G. and Dancer, S.J. and Kurnitski, J. and Li, Y. and Loomans, M.G.L.C. and Marr, L.C. and Morawska, L. and Nazaroff, W. and Noakes, C. and Querol, X. and Sekhar, C. and Tellier, R. and Greenhalgh, T. and Bourouiba, L. and Boerstra, A. and Tang, J.W. and Miller, S.L. and Jimenez, J.L.},
  date = {2022-01-05},
  journaltitle = {Environmental Science \& Technology},
  shortjournal = {Environ. Sci. Technol.},
  publisher = {{American Chemical Society}},
  issn = {0013-936X},
  doi = {10.1021/acs.est.1c06531},
  url = {https://doi.org/10.1021/acs.est.1c06531},
  urldate = {2022-01-13},
  abstract = {Some infectious diseases, including COVID-19, can undergo airborne transmission. This may happen at close proximity, but as time indoors increases, infections can occur in shared room air despite distancing. We propose two indicators of infection risk for this situation, that is, relative risk parameter (Hr) and risk parameter (H). They combine the key factors that control airborne disease transmission indoors: virus-containing aerosol generation rate, breathing flow rate, masking and its quality, ventilation and aerosol-removal rates, number of occupants, and duration of exposure. COVID-19 outbreaks show a clear trend that is consistent with airborne infection and enable recommendations to minimize transmission risk. Transmission in typical prepandemic indoor spaces is highly sensitive to mitigation efforts. Previous outbreaks of measles, influenza, and tuberculosis were also assessed. Measles outbreaks occur at much lower risk parameter values than COVID-19, while tuberculosis outbreaks are observed at higher risk parameter values. Because both diseases are accepted as airborne, the fact that COVID-19 is less contagious than measles does not rule out airborne transmission. It is important that future outbreak reports include information on masking, ventilation and aerosol-removal rates, number of occupants, and duration of exposure, to investigate airborne transmission.},
  file = {/home/ahmad/Zotero/storage/DL4S23LE/Peng et al. - 2022 - Practical Indicators for Risk of Airborne Transmis.pdf;/home/ahmad/Zotero/storage/7BEFKRAV/acs.est.html}
}

@unpublished{raschkaModelEvaluationModel2020,
  title = {Model {{Evaluation}}, {{Model Selection}}, and {{Algorithm Selection}} in {{Machine Learning}}},
  author = {Raschka, Sebastian},
  date = {2020-11-10},
  eprint = {1811.12808},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.12808},
  urldate = {2022-01-12},
  abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/ULLTQWEF/Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf;/home/ahmad/Zotero/storage/WJBTTTN7/1811.html}
}

@online{ResidualBootstrapsRegression,
  title = {Residual Bootstraps for Regression Model Validation | {{Science World Journal}}},
  url = {https://www.ajol.info/index.php/swj/article/view/208605},
  urldate = {2022-06-27},
  file = {/home/ahmad/Zotero/storage/K9JDFSFS/208605.html}
}

@article{rudinWhyAreWe2019,
  title = {Why {{Are We Using Black Box Models}} in {{AI When We Don}}’t {{Need To}}? {{A Lesson From}} an {{Explainable AI Competition}}},
  shorttitle = {Why {{Are We Using Black Box Models}} in {{AI When We Don}}’t {{Need To}}?},
  author = {Rudin, Cynthia and Radin, Joanna},
  date = {2019-11-22},
  journaltitle = {Harvard Data Science Review},
  volume = {1},
  number = {2},
  doi = {10.1162/99608f92.5a8a3a3d},
  url = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/6},
  urldate = {2022-04-07},
  abstract = {In 2018, a landmark challenge in artificial intelligence (AI) took place, namely, the Explainable Machine Learning Challenge. The goal of the competition was to create a complicated black box model for the dataset and explain how it worked. One team did not follow the rules. Instead of sending in a black box, they created a model that was fully interpretable. This leads to the question of whether the real world of machine learning is similar to the Explainable Machine Learning Challenge, where black box models are used even when they are not needed. We discuss this team’s thought processes during the competition and their implications, which reach far beyond the competition itself.Keywords: interpretability, explainability, machine learning, finance},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/HN252VW8/Rudin and Radin - 2019 - Why Are We Using Black Box Models in AI When We Do.pdf}
}

@article{siebertIntegratedBiomarkerDiscovery2011,
  title = {Integrated Biomarker Discovery: Combining Heterogeneous Data},
  shorttitle = {Integrated Biomarker Discovery},
  author = {Siebert, Janet},
  date = {2011-11-01},
  journaltitle = {Bioanalysis},
  volume = {3},
  number = {21},
  pages = {2369--2372},
  publisher = {{Future Science}},
  issn = {1757-6180},
  doi = {10.4155/bio.11.229},
  url = {https://www.future-science.com/doi/10.4155/bio.11.229},
  urldate = {2022-01-12},
  keywords = {data integration,data mining,data warehouse,databases,integrated biomarkers,OLAP,systems biology},
  file = {/home/ahmad/Zotero/storage/TAQ7LGN8/Siebert - 2011 - Integrated biomarker discovery combining heteroge.pdf}
}

@article{smithEstimationBrainAge2019,
  title = {Estimation of Brain Age Delta from Brain Imaging},
  author = {Smith, Stephen M. and Vidaurre, Diego and Alfaro-Almagro, Fidel and Nichols, Thomas E. and Miller, Karla L.},
  date = {2019-10-15},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {200},
  pages = {528--539},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.06.017},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811919305026},
  urldate = {2022-01-12},
  abstract = {It is of increasing interest to study “brain age” - the apparent age of a subject, as inferred from brain imaging data. The difference between brain age and actual age (the “delta”) is typically computed, reflecting deviation from the population norm. This therefore may reflect accelerated aging (positive delta) or resilience (negative delta) and has been found to be a useful correlate with factors such as disease and cognitive decline. However, although there has been a range of methods proposed for estimating brain age, there has been little study of the optimal ways of computing the delta. In this technical note we describe problems with the most common current approach, and present potential improvements. We evaluate different estimation methods on simulated and real data. We also find the strongest correlations of corrected brain age delta with 5,792 non-imaging variables (non-brain physical measures, life-factor measures, cognitive test scores, etc.), and also with 2,641 multimodal brain imaging-derived phenotypes, with data from 19,000 participants in UK Biobank.},
  langid = {english},
  keywords = {Brain aging,Brain imaging,UK biobank},
  file = {/home/ahmad/Zotero/storage/IXZG9TI3/Smith et al. - 2019 - Estimation of brain age delta from brain imaging.pdf;/home/ahmad/Zotero/storage/QD5JZQQ5/S1053811919305026.html}
}

@online{StatisticalLearningSelective,
  title = {Statistical Learning and Selective Inference | {{PNAS}}},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1507583112},
  urldate = {2022-06-28},
  file = {/home/ahmad/Zotero/storage/6NIHG8JN/pnas.html}
}

@article{stiglerCorrelationCausationComment2005,
  title = {Correlation and Causation: A Comment},
  shorttitle = {Correlation and Causation},
  author = {Stigler, Stephen},
  date = {2005},
  journaltitle = {Perspectives in Biology and Medicine},
  shortjournal = {Perspect Biol Med},
  volume = {48},
  eprint = {15842089},
  eprinttype = {pmid},
  pages = {S88-94},
  issn = {0031-5982},
  abstract = {Some purely methodological comments are made on the pitfalls and difficulties in making causal inferences from observational data, including in studies of disparity in medicine. The ideas of spurious correlation and measurement error are discussed with an eye towards their impact upon inferences about causality, and cautions are offered about over-reliance upon testing hypotheses.},
  issue = {1 Suppl},
  langid = {english},
  keywords = {Bias,Causality,Health Services Research}
}

@article{stroblConditionalVariableImportance2008,
  title = {Conditional Variable Importance for Random Forests},
  author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
  date = {2008-07-11},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {9},
  number = {1},
  pages = {307},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-307},
  url = {https://doi.org/10.1186/1471-2105-9-307},
  urldate = {2022-01-12},
  abstract = {Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.},
  langid = {english},
  file = {/home/ahmad/Zotero/storage/ML7VF5ZJ/Strobl et al. - 2008 - Conditional variable importance for random forests.pdf}
}

@article{sudlowUKBiobankOpen2015,
  title = {{{UK Biobank}}: {{An Open Access Resource}} for {{Identifying}} the {{Causes}} of a {{Wide Range}} of {{Complex Diseases}} of {{Middle}} and {{Old Age}}},
  shorttitle = {{{UK Biobank}}},
  author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins, Rory},
  date = {2015-03-31},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {12},
  number = {3},
  pages = {e1001779},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001779},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001779},
  urldate = {2022-06-28},
  abstract = {Cathie Sudlow and colleagues describe the UK Biobank, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
  langid = {english},
  keywords = {Cohort studies,Global health,Intelligence tests,Magnetic resonance imaging,Prospective studies,Questionnaires,Research ethics,Scientists},
  file = {/home/ahmad/Zotero/storage/FAPBG55A/Sudlow et al. - 2015 - UK Biobank An Open Access Resource for Identifyin.pdf}
}

@unpublished{suteraGlobalLocalMDI2021,
  title = {From Global to Local {{MDI}} Variable Importances for Random Forests and When They Are {{Shapley}} Values},
  author = {Sutera, Antonio and Louppe, Gilles and Huynh-Thu, Van Anh and Wehenkel, Louis and Geurts, Pierre},
  date = {2021-11-03},
  eprint = {2111.02218},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.02218},
  urldate = {2022-01-13},
  abstract = {Random forests have been widely used for their ability to provide so-called importance measures, which give insight at a global (per dataset) level on the relevance of input variables to predict a certain output. On the other hand, methods based on Shapley values have been introduced to refine the analysis of feature relevance in tree-based models to a local (per instance) level. In this context, we first show that the global Mean Decrease of Impurity (MDI) variable importance scores correspond to Shapley values under some conditions. Then, we derive a local MDI importance measure of variable relevance, which has a very natural connection with the global MDI measure and can be related to a new notion of local feature relevance. We further link local MDI importances with Shapley values and discuss them in the light of related measures from the literature. The measures are illustrated through experiments on several classification and regression problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ahmad/Zotero/storage/BJL94ZBT/Sutera et al. - 2021 - From global to local MDI variable importances for .pdf;/home/ahmad/Zotero/storage/FMCHZ7DL/2111.html}
}

@article{taylorStatisticalLearningSelective2015,
  title = {Statistical Learning and Selective Inference},
  author = {Taylor, Jonathan and Tibshirani, Robert J.},
  date = {2015-06-23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {25},
  pages = {7629--7634},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1507583112},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1507583112},
  urldate = {2022-06-28},
  file = {/home/ahmad/Zotero/storage/TCQ8HDHM/Taylor and Tibshirani - 2015 - Statistical learning and selective inference.pdf}
}

@online{UKBiobankData,
  title = {{{UK Biobank Data}}: {{Come}} and {{Get It}}},
  url = {https://www.science.org/doi/full/10.1126/scitranslmed.3008601},
  urldate = {2022-01-12},
  file = {/home/ahmad/Zotero/storage/5EFLAXX2/scitranslmed.html}
}

@article{vanschaikExplainableStatisticalLearning2019,
  title = {Explainable Statistical Learning in Public Health for Policy Development: The Case of Real-World Suicide Data},
  shorttitle = {Explainable Statistical Learning in Public Health for Policy Development},
  author = {van Schaik, Paul and Peng, Yonghong and Ojelabi, Adedokun and Ling, Jonathan},
  options = {useprefix=true},
  date = {2019-07-17},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {152},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0796-7},
  url = {https://doi.org/10.1186/s12874-019-0796-7},
  urldate = {2022-06-28},
  abstract = {In recent years, the availability of publicly available data related to public health has significantly increased. These data have substantial potential to develop public health policy; however, this requires meaningful and insightful analysis. Our aim is to demonstrate how data analysis techniques can be used to address the issues of data reduction, prediction and explanation using online available public health data, in order to provide a sound basis for informing public health policy.},
  keywords = {Explainability,Feature reduction,Health informatics,Interpretability,Machine learning,Mental health,Public health data,Statistical learning,Suicide},
  file = {/home/ahmad/Zotero/storage/2P26GAA6/van Schaik et al. - 2019 - Explainable statistical learning in public health .pdf}
}

@article{vidal-pineiroIndividualVariationsBrain2021,
  title = {Individual Variations in ‘Brain Age’ Relate to Early-Life Factors More than to Longitudinal Brain Change},
  author = {Vidal-Pineiro, Didac and Wang, Yunpeng and Krogsrud, Stine K and Amlien, Inge K and Baaré, William FC and Bartres-Faz, David and Bertram, Lars and Brandmaier, Andreas M and Drevon, Christian A and Düzel, Sandra and Ebmeier, Klaus and Henson, Richard N and Junqué, Carme and Kievit, Rogier Andrew and Kühn, Simone and Leonardsen, Esten and Lindenberger, Ulman and Madsen, Kathrine S and Magnussen, Fredrik and Mowinckel, Athanasia Monika and Nyberg, Lars and Roe, James M and Segura, Barbara and Smith, Stephen M and Sørensen, Øystein and Suri, Sana and Westerhausen, Rene and Zalesky, Andrew and Zsoldos, Enikő and Walhovd, Kristine Beate and Fjell, Anders},
  editor = {Zhou, Juan and Büchel, Christian and Zuo, Xi-Nian},
  date = {2021-11-10},
  journaltitle = {eLife},
  volume = {10},
  pages = {e69995},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.69995},
  url = {https://doi.org/10.7554/eLife.69995},
  urldate = {2022-01-13},
  abstract = {Brain age is a widely used index for quantifying individuals’ brain health as deviation from a normative brain aging trajectory. Higher-than-expected brain age is thought partially to reflect above-average rate of brain aging. Here, we explicitly tested this assumption in two independent large test datasets (UK Biobank [main] and Lifebrain [replication]; longitudinal observations ≈ 2750 and 4200) by assessing the relationship between cross-sectional and longitudinal estimates of brain age. Brain age models were estimated in two different training datasets (n ≈ 38,000 [main] and 1800 individuals [replication]) based on brain structural features. The results showed no association between cross-sectional brain age and the rate of brain change measured longitudinally. Rather, brain age in adulthood was associated with the congenital factors of birth weight and polygenic scores of brain age, assumed to reflect a constant, lifelong influence on brain structure from early life. The results call for nuanced interpretations of cross-sectional indices of the aging brain and question their validity as markers of ongoing within-person changes of the aging brain. Longitudinal imaging data should be preferred whenever the goal is to understand individual change trajectories of brain and cognition in aging.},
  keywords = {Aging,Brain age delta,brain age gap,brain decline,neuroimaging,T1w},
  file = {/home/ahmad/Zotero/storage/8DTYU6UY/Vidal-Pineiro et al. - 2021 - Individual variations in ‘brain age’ relate to ear.pdf}
}

@article{wuElectroencephalographicSignaturePredicts2020,
  title = {An Electroencephalographic Signature Predicts Antidepressant Response in Major Depression},
  author = {Wu, Wei and Zhang, Yu and Jiang, Jing and Lucas, Molly V. and Fonzo, Gregory A. and Rolle, Camarin E. and Cooper, Crystal and Chin-Fatt, Cherise and Krepel, Noralie and Cornelssen, Carena A. and Wright, Rachael and Toll, Russell T. and Trivedi, Hersh M. and Monuszko, Karen and Caudle, Trevor L. and Sarhadi, Kamron and Jha, Manish K. and Trombello, Joseph M. and Deckersbach, Thilo and Adams, Phil and McGrath, Patrick J. and Weissman, Myrna M. and Fava, Maurizio and Pizzagalli, Diego A. and Arns, Martijn and Trivedi, Madhukar H. and Etkin, Amit},
  date = {2020-04},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {38},
  number = {4},
  pages = {439--447},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-019-0397-3},
  url = {https://www.nature.com/articles/s41587-019-0397-3},
  urldate = {2022-03-17},
  abstract = {Antidepressants are widely prescribed, but their efficacy relative to placebo is modest, in part because the clinical diagnosis of major depression encompasses biologically heterogeneous conditions. Here, we sought to identify a neurobiological signature of response to antidepressant treatment as compared to placebo. We designed a latent-space machine-learning algorithm tailored for resting-state electroencephalography (EEG) and applied it to data from the largest imaging-coupled, placebo-controlled antidepressant study (n\,=\,309). Symptom improvement was robustly predicted in a manner both specific for the antidepressant sertraline (versus placebo) and generalizable across different study sites and EEG equipment. This sertraline-predictive EEG signature generalized to two depression samples, wherein it reflected general antidepressant medication responsivity and related differentially to a repetitive transcranial magnetic stimulation treatment outcome. Furthermore, we found that the sertraline resting-state EEG signature indexed prefrontal neural responsivity, as measured by concurrent transcranial magnetic stimulation and EEG. Our findings advance the neurobiological understanding of antidepressant treatment through an EEG-tailored computational model and provide a clinical avenue for personalized treatment of depression.},
  issue = {4},
  langid = {english},
  keywords = {Brain imaging,Computational neuroscience,Depression,Predictive markers},
  file = {/home/ahmad/Zotero/storage/XSV8H4AR/Wu et al. - 2020 - An electroencephalographic signature predicts anti.pdf;/home/ahmad/Zotero/storage/F3TBMT33/s41587-019-0397-3.html}
}

@inproceedings{yeHeterogeneousDataFusion2008,
  title = {Heterogeneous Data Fusion for Alzheimer's Disease Study},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Ye, Jieping and Chen, Kewei and Wu, Teresa and Li, Jing and Zhao, Zheng and Patel, Rinkal and Bae, Min and Janardan, Ravi and Liu, Huan and Alexander, Gene and Reiman, Eric},
  date = {2008-08-24},
  series = {{{KDD}} '08},
  pages = {1025--1033},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1401890.1402012},
  url = {https://doi.org/10.1145/1401890.1402012},
  urldate = {2022-01-12},
  abstract = {Effective diagnosis of Alzheimer's disease (AD) is of primary importance in biomedical research. Recent studies have demonstrated that neuroimaging parameters are sensitive and consistent measures of AD. In addition, genetic and demographic information have also been successfully used for detecting the onset and progression of AD. The research so far has mainly focused on studying one type of data source only. It is expected that the integration of heterogeneous data (neuroimages, demographic, and genetic measures) will improve the prediction accuracy and enhance knowledge discovery from the data, such as the detection of biomarkers. In this paper, we propose to integrate heterogeneous data for AD prediction based on a kernel method. We further extend the kernel framework for selecting features (biomarkers) from heterogeneous data sources. The proposed method is applied to a collection of MRI data from 59 normal healthy controls and 59 AD patients. The MRI data are pre-processed using tensor factorization. In this study, we treat the complementary voxel-based data and region of interest (ROI) data from MRI as two data sources, and attempt to integrate the complementary information by the proposed method. Experimental results show that the integration of multiple data sources leads to a considerable improvement in the prediction accuracy. Results also show that the proposed algorithm identifies biomarkers that play more significant roles than others in AD diagnosis.},
  isbn = {978-1-60558-193-4},
  keywords = {biomarker detection,heterogeneous data source fusion,multiple kernel learning,neuroimaging,tensor factorization}
}

@article{zhengSummarizingPredictivePower2000,
  title = {Summarizing the Predictive Power of a Generalized Linear Model},
  author = {Zheng, Beiyao and Agresti, Alan},
  date = {2000},
  journaltitle = {Statistics in Medicine},
  volume = {19},
  number = {13},
  pages = {1771--1781},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20000715)19:13<1771::AID-SIM485>3.0.CO;2-P},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-0258%2820000715%2919%3A13%3C1771%3A%3AAID-SIM485%3E3.0.CO%3B2-P},
  urldate = {2022-06-28},
  abstract = {This paper studies summary measures of the predictive power of a generalized linear model, paying special attention to a generalization of the multiple correlation coefficient from ordinary linear regression. The population value is the correlation between the response and its conditional expectation given the predictors, and the sample value is the correlation between the observed response and the model predicted value. We compare four estimators of the measure in terms of bias, mean squared error and behaviour in the presence of overparameterization. The sample estimator and a jack-knife estimator usually behave adequately, but a cross-validation estimator has a large negative bias with large mean squared error. One can use bootstrap methods to construct confidence intervals for the population value of the correlation measure and to estimate the degree to which a model selection procedure may provide an overly optimistic measure of the actual predictive power. Copyright © 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-0258\%2820000715\%2919\%3A13\%3C1771\%3A\%3AAID-SIM485\%3E3.0.CO\%3B2-P},
  file = {/home/ahmad/Zotero/storage/JJDSN9CB/Zheng and Agresti - 2000 - Summarizing the predictive power of a generalized .pdf}
}


